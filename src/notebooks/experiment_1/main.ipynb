{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root_path=PosixPath('/mnt/storage_2/scratch/pl0145-01/jsmok/qa/mt')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from typing import Generator, Dict, Any\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "\n",
    "root_path = Path.cwd().parent.parent.parent\n",
    "root_path = root_path\n",
    "print(f\"{root_path=}\")\n",
    "sys.path.append(str(root_path))\n",
    "load_dotenv(dotenv_path=root_path / \"src\" / \"config\" / \".env\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '2-0', 'title': 'AWK', 'text': 'AWK – interpretowany język programowania, którego główną funkcją jest wyszukiwanie i przetwarzanie wzorców w plikach lub strumieniach danych. Jest także nazwą programu początkowo dostępnego dla systemów operacyjnych będących pochodnymi UNIX-a, obecnie także na inne platformy. AWK jest językiem, który w znacznym stopniu wykorzystuje tablice asocjacyjne, stringi i wyrażenia regularne.'}\n"
     ]
    }
   ],
   "source": [
    "# Generator function to read and process a JSONL file line by line\n",
    "def read_jsonl_file(file_path: Path) -> Generator[Dict[str, Any], None, None]:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Parse the JSON object from each line\n",
    "            json_obj = json.loads(line.strip())\n",
    "            yield json_obj\n",
    "\n",
    "# Example usage\n",
    "passages_path = root_path / \"data\" / \"ipipan-polqa\"/ \"passages.jsonl\"\n",
    "for json_obj in read_jsonl_file(passages_path):\n",
    "    print(json_obj)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config.configs import get_config\n",
    "config = get_config(\"config1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lib.VectorDb.tools import get_vector_db\n",
    "vector_db = get_vector_db(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 17:54:29,509 - INFO - device: cuda\n",
      "/mnt/storage_2/scratch/pl0145-01/jsmok/qa/mt/venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/mnt/storage_2/scratch/pl0145-01/jsmok/qa/mt/venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2024-06-12 17:54:32,398 - INFO - Load pretrained SentenceTransformer: intfloat/multilingual-e5-large\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebcd138e59d24bcf84c1cd2654818b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba8b9910feb46c291c4346b2892d9fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/160k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd67542ff5174d189a9af8aeceab9e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storage_2/scratch/pl0145-01/jsmok/qa/mt/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4785a2beccb3483caa205090905f983c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc48781265094ff79a7a3df847c8823d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b233470ce0a4be9aa2700f66f4c6e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab011898132f4915a16fc87bf570b30e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac53a8e19da4c3dace7c0dc2e46db0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fab5b35dc5544fea2a169546c928e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b84796d6dd6f41e6b330986a19d43421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/201 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.lib.Embeddings.tools import get_embedding_model\n",
    "embeddings = get_embedding_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_jsonl(file_path: str) -> tuple[tuple[dict[str, str]], tuple[str], tuple[int]]:\n",
    "    id_title_list = []\n",
    "    text_list = []\n",
    "    len_list = []\n",
    "    \n",
    "    num_lines = sum(1 for _ in open(file_path, 'r', encoding='utf-8'))\n",
    "    i=0\n",
    "    for json_obj in tqdm(read_jsonl_file(file_path), total=num_lines, desc=\"Processing JSONL\"):\n",
    "        if 'id' in json_obj and 'title' in json_obj and 'text' in json_obj:\n",
    "            id_title_list.append({\"id\": json_obj['id'], \"title\": json_obj['title']})\n",
    "            text_list.append(json_obj['text'])\n",
    "            len_list.append(len(json_obj['text']))\n",
    "        i+=1\n",
    "        if i > 150:\n",
    "            break\n",
    "\n",
    "    combined_list = list(zip(id_title_list, text_list, len_list))\n",
    "    sorted_combined_list = sorted(combined_list, key=lambda x: x[2])\n",
    "    id_title_list, text_list, len_list = zip(*sorted_combined_list)\n",
    "    \n",
    "    \n",
    "    return id_title_list, text_list, len_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSONL:   0%|          | 150/7097288 [00:00<01:13, 97045.44it/s]\n"
     ]
    }
   ],
   "source": [
    "meta, docs, lens = process_jsonl(passages_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 151\n",
      "Number of metadata: 151\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"data\"\n",
    "SPLIT_INTO_CHUNKS = True\n",
    "MAX_CHUNK_SIZE = 512\n",
    "CHUNK_OVERLAP = 100\n",
    "TEXT_SPLITTER_SEPARATOR = \"\"\n",
    "VECTOR_DB_PATH = \"vector_db\"\n",
    "\n",
    "MODEL_ID = \"hf-e5\"\n",
    "VECTOR_DB_ID = \"faiss\"\n",
    "\n",
    "import abc\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "from functools import lru_cache\n",
    "\n",
    "import torch\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "\n",
    "\n",
    "    # Read docs\n",
    "\n",
    "\n",
    "# split into chunks (if needed)\n",
    "if SPLIT_INTO_CHUNKS:\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        chunk_size=MAX_CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP, separator=TEXT_SPLITTER_SEPARATOR\n",
    "    )\n",
    "    docs_chunks = text_splitter.create_documents(docs, meta)\n",
    "    docs = [doc.page_content for doc in docs_chunks]\n",
    "    metadata = [doc.metadata for doc in docs_chunks]\n",
    "print(f\"Number of documents: {len(docs)}\" )\n",
    "print(f\"Number of metadata: {len(meta)}\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate embeddings\n",
    "embedding_model = get_embedding_model(config)\n",
    "embeddings = embedding_model.embed_documents(docs)\n",
    "docs_embeddings_pairs = list(zip(docs, embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store data in vector db\n",
    "vector_db = get_vector_db(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 23:38:44,062 - WARNING - `embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    }
   ],
   "source": [
    "vector_db.store_embeddings(docs_embeddings_pairs, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FAISSVectorDb' object has no attribute 'index_to_docstore_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mvector_db\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_to_docstore_id\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FAISSVectorDb' object has no attribute 'index_to_docstore_id'"
     ]
    }
   ],
   "source": [
    "len(vector_db.index_to_docstore_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/mnt/storage_2/scratch/pl0145-01/jsmok/data/vector_databases/faiss_vector_db_config_1')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db.vector_db_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of indexed vectors: 151\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# Load the FAISS index from the .faiss file\n",
    "index = faiss.read_index(\"/mnt/storage_2/scratch/pl0145-01/jsmok/data/vector_databases/faiss_vector_db_config_1/index.faiss\")\n",
    "\n",
    "# Get the number of indexed vectors\n",
    "num_vectors = index.ntotal\n",
    "print(f\"Number of indexed vectors: {num_vectors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lib.data_tools.read_file import process_jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config.Config import Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lib.common.tools import sort_docs_by_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lib.VectorDb.tools import get_vector_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lib.Embeddings.tools import get_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
